# Project: Data Modeling with Postgres

### Introduction
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They'd like a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis. The goal is to create a database schema and ETL pipeline for this analysis

###  Data
The datasets used for this project are located under the data folder. The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/) and is contained in the song_data subfolder. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 

The second dataset is located under the log_data subfolder and consists of log files in JSON format generated by an [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The log files in the dataset you'll be working with are partitioned by year and month.

### Schema for Song Play Analysis
Using the song and log datasets, we create a star schema optimized for queries on song play analysis. This includes the following tables:

**Fact Table**
songplays - records in log data associated with song plays i.e. records with page NextSong
*songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

**Dimension Tables**
users - users in the app
*user_id, first_name, last_name, gender, level*

songs - songs in music database
*song_id, title, artist_id, year, duration*

artists - artists in music database
*artist_id, name, location, latitude, longitude*

time - timestamps of records in songplays broken down into specific units
*start_time, hour, day, week, month, year, weekday*

### File Descritions
1. `test.ipynb` displays the first few rows of each table as a check for the created database.
2. `create_tables.py` drops and creates the fact and dimension tables. This file is run to reset the tables before running the ETL scripts.
3. `etl.ipynb` reads and processes a single file from song_data and log_data and loads the data into your tables. This is an interactive notebook used to develop the logic in `etl.py`
3. `etl.py` reads and processes files from `song_data` and `log_data` and loads them into the fact and dimension tables described above
4. `sql_queries.py` contains all of the sql queries and is imported into the last three files above

### ETL Pipeline
1. Connect to the sparkify database
2. Collect all songs found under `/data/song_data`, and for each JSON file found we call the `process_song_file` function
3. Select fields of interest for use in populating the songs and artists tables:
```artist_select_cols = ["artist_id", "artist_name", "artist_location", "artist_latitude", "artist_longitude"]```
```song_select_cols = ["song_id", "title", "artist_id", "year", "duration"]``` and insert into the songs and artists tables row by row
4. Collect all log files found under `/data/log_data`, and for each JSON file found we call the `process_log_file` function
5. Select only those rows where page = 'NextSong'
6. Convert the `ts` column which is in milliseconds to a datetime format. We obtain the parameters we need from this date (day, hour, week, etc), and insert everything into our time dimention table
7. Load user data into our users table
8. The last step is to lookup the `song_id` and `artist_id` from their tables by searching for matches on the song name, artist name and song duration that we have in the song play data logs. The query used that we use to accomplish this is the following:
``` 
SELECT
        songs.song_id AS song_id,
        artists.artist_id AS artist_id
    FROM songs JOIN artists ON songs.artist_id = artists.artist_id
    WHERE songs.title = %s
    AND artists.name = %s
    AND songs.duration = %s
```
9. With the song_id and artist_id  found from step 8 above, we then use this together with additional information from the row in the logs to insert: timestamp, userId, level, songid, artistid, sessionId, location and userAgent into the songplays fact table row by row


### Usage
To run the entire ETL to populate the fact and dimension tables run:
`python create_tables.py`
followed by 
`python etl.py`
Confirm that the tables are populated correctly in each of the fact and dimension tables under the schema defined in `sql_queries.py` by running the notebook `test.ipynb` 

### Sanity Check
Confirm that the query 
`SELECT * FROM songplays WHERE artistid IS NOT NULL AND songid IS NOT NULL LIMIT 5` returns a single row as expected in the project rubric 

### Author
Michael Glaros [Github](https://github.com/mglaros)